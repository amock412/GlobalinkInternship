{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification of Park Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkReviews = pd.read_csv('ParkReviewsLang.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41822, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parkReviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_for</th>\n",
       "      <th>review_id</th>\n",
       "      <th>username</th>\n",
       "      <th>user_url</th>\n",
       "      <th>published</th>\n",
       "      <th>date_retrieved</th>\n",
       "      <th>num_stars</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>review_text</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Parc de la Capture-d'Ethan-Allen</td>\n",
       "      <td>ChdDSUhNMG9nS0VJQ0FnSUNpeGF6TTNnRRAB</td>\n",
       "      <td>Claudia</td>\n",
       "      <td>https://www.google.com/maps/contrib/1001449741...</td>\n",
       "      <td>7 months ago</td>\n",
       "      <td>2021-06-20 22:04:09.211296</td>\n",
       "      <td>4.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>One of the nicest entry points to this invitin...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Parc de la Capture-d'Ethan-Allen</td>\n",
       "      <td>ChdDSUhNMG9nS0VJQ0FnSURDOGEyMGpnRRAB</td>\n",
       "      <td>Nate Neel</td>\n",
       "      <td>https://www.google.com/maps/contrib/1121030547...</td>\n",
       "      <td>8 months ago</td>\n",
       "      <td>2021-06-20 22:04:09.212245</td>\n",
       "      <td>5.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>Waterfront to fish or just relax, great place ...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Parc de la Capture-d'Ethan-Allen</td>\n",
       "      <td>ChdDSUhNMG9nS0VJQ0FnSUM4Nk9Ya3lnRRAB</td>\n",
       "      <td>Yucel Salimoglu</td>\n",
       "      <td>https://www.google.com/maps/contrib/1034180738...</td>\n",
       "      <td>11 months ago</td>\n",
       "      <td>2021-06-20 22:04:09.213178</td>\n",
       "      <td>4.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>Everything except the parking is good here.</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parc de la Capture-d'Ethan-Allen</td>\n",
       "      <td>ChZDSUhNMG9nS0VJQ0FnSUNVdWNUbE9REAE</td>\n",
       "      <td>COCO BEADZ</td>\n",
       "      <td>https://www.google.com/maps/contrib/1036060504...</td>\n",
       "      <td>a year ago</td>\n",
       "      <td>2021-06-20 22:04:09.214115</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>Defenely the best park in Montreal East, Tetre...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parc de la Capture-d'Ethan-Allen</td>\n",
       "      <td>ChdDSUhNMG9nS0VJQ0FnSUMwdHJDTm1nRRAB</td>\n",
       "      <td>Anna Maria Fiore</td>\n",
       "      <td>https://www.google.com/maps/contrib/1016779009...</td>\n",
       "      <td>a year ago</td>\n",
       "      <td>2021-06-20 22:04:09.215069</td>\n",
       "      <td>5.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>It's so peaceful and happy place near the water</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         review_for                             review_id  \\\n",
       "0  Parc de la Capture-d'Ethan-Allen  ChdDSUhNMG9nS0VJQ0FnSUNpeGF6TTNnRRAB   \n",
       "1  Parc de la Capture-d'Ethan-Allen  ChdDSUhNMG9nS0VJQ0FnSURDOGEyMGpnRRAB   \n",
       "2  Parc de la Capture-d'Ethan-Allen  ChdDSUhNMG9nS0VJQ0FnSUM4Nk9Ya3lnRRAB   \n",
       "3  Parc de la Capture-d'Ethan-Allen   ChZDSUhNMG9nS0VJQ0FnSUNVdWNUbE9REAE   \n",
       "4  Parc de la Capture-d'Ethan-Allen  ChdDSUhNMG9nS0VJQ0FnSUMwdHJDTm1nRRAB   \n",
       "\n",
       "           username                                           user_url  \\\n",
       "0           Claudia  https://www.google.com/maps/contrib/1001449741...   \n",
       "1         Nate Neel  https://www.google.com/maps/contrib/1121030547...   \n",
       "2   Yucel Salimoglu  https://www.google.com/maps/contrib/1034180738...   \n",
       "3        COCO BEADZ  https://www.google.com/maps/contrib/1036060504...   \n",
       "4  Anna Maria Fiore  https://www.google.com/maps/contrib/1016779009...   \n",
       "\n",
       "       published              date_retrieved  num_stars  num_reviews  \\\n",
       "0   7 months ago  2021-06-20 22:04:09.211296        4.0        107.0   \n",
       "1   8 months ago  2021-06-20 22:04:09.212245        5.0        121.0   \n",
       "2  11 months ago  2021-06-20 22:04:09.213178        4.0         79.0   \n",
       "3     a year ago  2021-06-20 22:04:09.214115        4.0        128.0   \n",
       "4     a year ago  2021-06-20 22:04:09.215069        5.0         39.0   \n",
       "\n",
       "                                         review_text  label lang  \n",
       "0  One of the nicest entry points to this invitin...      1   en  \n",
       "1  Waterfront to fish or just relax, great place ...      1   en  \n",
       "2        Everything except the parking is good here.      1   en  \n",
       "3  Defenely the best park in Montreal East, Tetre...      1   en  \n",
       "4    It's so peaceful and happy place near the water      1   en  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parkReviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17149, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frenchReviews = parkReviews[parkReviews['lang'] == 'fr']\n",
    "frenchReviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frenchReviewsDf = frenchReviews.copy()\n",
    "frenchReviewText = frenchReviews['review_text'].apply(lambda x: x.split('(Original)')[-1].strip())\n",
    "frenchReviewsDf['french_text'] = frenchReviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_for</th>\n",
       "      <th>review_id</th>\n",
       "      <th>username</th>\n",
       "      <th>user_url</th>\n",
       "      <th>published</th>\n",
       "      <th>date_retrieved</th>\n",
       "      <th>num_stars</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>review_text</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "      <th>french_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22684</th>\n",
       "      <td>Parc de la Capture-d'Ethan-Allen</td>\n",
       "      <td>ChdDSUhNMG9nS0VJQ0FnSUNxM3JIQjhBRRAB</td>\n",
       "      <td>Claude Gagnon</td>\n",
       "      <td>https://www.google.com/maps/contrib/1182846684...</td>\n",
       "      <td>a week ago</td>\n",
       "      <td>2021-06-20 22:04:09.230541</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>(Translated by Google) Very beautiful park to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>Tres beau parc pour faire un pinic et profiter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22685</th>\n",
       "      <td>Parc de la Capture-d'Ethan-Allen</td>\n",
       "      <td>ChdDSUhNMG9nS0VJQ0FnSURLckpMTm5nRRAB</td>\n",
       "      <td>Guy Durand</td>\n",
       "      <td>https://www.google.com/maps/contrib/1056233036...</td>\n",
       "      <td>a month ago</td>\n",
       "      <td>2021-06-20 22:04:09.232736</td>\n",
       "      <td>5.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>(Translated by Google) The people are all very...</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>Les gens sont tous très sociables.\\nExceptionnel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22686</th>\n",
       "      <td>Parc de la Capture-d'Ethan-Allen</td>\n",
       "      <td>ChZDSUhNMG9nS0VJQ0FnSUNBNktMclNREAE</td>\n",
       "      <td>Dania Pascual</td>\n",
       "      <td>https://www.google.com/maps/contrib/1064940459...</td>\n",
       "      <td>2 years ago</td>\n",
       "      <td>2021-06-20 22:04:09.233660</td>\n",
       "      <td>5.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>(Translated by Google) Nice place to walk, jog...</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>Belle place pour marcher, jogger, promener le ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22687</th>\n",
       "      <td>Parc de la Capture-d'Ethan-Allen</td>\n",
       "      <td>ChdDSUhNMG9nS0VJQ0FnSUNxNWFEUzBBRRAB</td>\n",
       "      <td>Stéphane Lessard</td>\n",
       "      <td>https://www.google.com/maps/contrib/1061684432...</td>\n",
       "      <td>a week ago</td>\n",
       "      <td>2021-06-20 22:04:09.234580</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>(Translated by Google) Excellent food!\\n\\n(Ori...</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>Nourriture excellente !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22688</th>\n",
       "      <td>Parc de la Capture-d'Ethan-Allen</td>\n",
       "      <td>ChZDSUhNMG9nS0VJQ0FnSUNLaTZ2YVFBEAE</td>\n",
       "      <td>Pitchou Kasongo</td>\n",
       "      <td>https://www.google.com/maps/contrib/1176531246...</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>2021-06-20 22:04:09.235577</td>\n",
       "      <td>4.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>(Translated by Google) I like to get some fres...</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>J aime bien pour prendre de l air frais</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             review_for                             review_id  \\\n",
       "22684  Parc de la Capture-d'Ethan-Allen  ChdDSUhNMG9nS0VJQ0FnSUNxM3JIQjhBRRAB   \n",
       "22685  Parc de la Capture-d'Ethan-Allen  ChdDSUhNMG9nS0VJQ0FnSURLckpMTm5nRRAB   \n",
       "22686  Parc de la Capture-d'Ethan-Allen   ChZDSUhNMG9nS0VJQ0FnSUNBNktMclNREAE   \n",
       "22687  Parc de la Capture-d'Ethan-Allen  ChdDSUhNMG9nS0VJQ0FnSUNxNWFEUzBBRRAB   \n",
       "22688  Parc de la Capture-d'Ethan-Allen   ChZDSUhNMG9nS0VJQ0FnSUNLaTZ2YVFBEAE   \n",
       "\n",
       "               username                                           user_url  \\\n",
       "22684     Claude Gagnon  https://www.google.com/maps/contrib/1182846684...   \n",
       "22685        Guy Durand  https://www.google.com/maps/contrib/1056233036...   \n",
       "22686     Dania Pascual  https://www.google.com/maps/contrib/1064940459...   \n",
       "22687  Stéphane Lessard  https://www.google.com/maps/contrib/1061684432...   \n",
       "22688   Pitchou Kasongo  https://www.google.com/maps/contrib/1176531246...   \n",
       "\n",
       "          published              date_retrieved  num_stars  num_reviews  \\\n",
       "22684    a week ago  2021-06-20 22:04:09.230541        4.0         86.0   \n",
       "22685   a month ago  2021-06-20 22:04:09.232736        5.0         53.0   \n",
       "22686   2 years ago  2021-06-20 22:04:09.233660        5.0         41.0   \n",
       "22687    a week ago  2021-06-20 22:04:09.234580        4.0          7.0   \n",
       "22688  2 months ago  2021-06-20 22:04:09.235577        4.0         93.0   \n",
       "\n",
       "                                             review_text  label lang  \\\n",
       "22684  (Translated by Google) Very beautiful park to ...      1   fr   \n",
       "22685  (Translated by Google) The people are all very...      1   fr   \n",
       "22686  (Translated by Google) Nice place to walk, jog...      1   fr   \n",
       "22687  (Translated by Google) Excellent food!\\n\\n(Ori...      1   fr   \n",
       "22688  (Translated by Google) I like to get some fres...      1   fr   \n",
       "\n",
       "                                             french_text  \n",
       "22684  Tres beau parc pour faire un pinic et profiter...  \n",
       "22685  Les gens sont tous très sociables.\\nExceptionnel.  \n",
       "22686  Belle place pour marcher, jogger, promener le ...  \n",
       "22687                            Nourriture excellente !  \n",
       "22688            J aime bien pour prendre de l air frais  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frenchReviewsDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating summary statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "frenchReviewsDf['word_count'] = [len(review.split()) for review in frenchReviewsDf['french_text']]\n",
    "\n",
    "frenchReviewsDf['uppercase_char_count'] = [sum(char.isupper() for char in review) \\\n",
    "                              for review in frenchReviewsDf['french_text']]                           \n",
    "\n",
    "frenchReviewsDf['special_char_count'] = [sum(char in string.punctuation for char in review) \\\n",
    "                            for review in frenchReviewsDf['french_text']]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews = frenchReviewsDf[frenchReviewsDf['label'] == 1]\n",
    "neg_reviews = frenchReviewsDf[frenchReviewsDf['label'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After breaking down the dataset into positive and negative reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14635.000000\n",
       "mean        12.701537\n",
       "std         16.127711\n",
       "min          1.000000\n",
       "25%          4.000000\n",
       "50%          8.000000\n",
       "75%         15.000000\n",
       "max        474.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_reviews['word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2514.000000\n",
       "mean       16.305091\n",
       "std        19.940490\n",
       "min         1.000000\n",
       "25%         5.000000\n",
       "50%        10.000000\n",
       "75%        20.000000\n",
       "max       258.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_reviews['word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8534025307598111"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14635/(14635 + 2514)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compare the number of uppercase letters used in the postive and negative reviews. As we can see there is no real difference between the two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14635.000000\n",
       "mean         1.819542\n",
       "std          2.511317\n",
       "min          0.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          2.000000\n",
       "max        107.000000\n",
       "Name: uppercase_char_count, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_reviews['uppercase_char_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2514.000000\n",
       "mean        2.029037\n",
       "std         3.391481\n",
       "min         0.000000\n",
       "25%         1.000000\n",
       "50%         1.000000\n",
       "75%         2.000000\n",
       "max        80.000000\n",
       "Name: uppercase_char_count, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_reviews['uppercase_char_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's take a look at the special characters present in the positive and negativve group of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14635.000000\n",
       "mean         2.345268\n",
       "std          3.710336\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          1.000000\n",
       "75%          3.000000\n",
       "max         94.000000\n",
       "Name: special_char_count, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_reviews['special_char_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2514.000000\n",
       "mean        2.945505\n",
       "std         4.274390\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         2.000000\n",
       "75%         4.000000\n",
       "max        63.000000\n",
       "Name: special_char_count, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_reviews['special_char_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andreamock/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostCommonWords(reviews, n_most_common, stopwords=None):\n",
    "\n",
    "    # flatten review column into a list of words, and set each to lowercase\n",
    "    flattened_reviews = [word for review in reviews for word in \\\n",
    "                         review.lower().split()]\n",
    "\n",
    "\n",
    "    # remove punctuation from reviews\n",
    "    flattened_reviews = [''.join(char for char in review if \\\n",
    "                                 char not in string.punctuation) for \\\n",
    "                         review in flattened_reviews]\n",
    "\n",
    "\n",
    "    # remove stopwords, if applicable\n",
    "    if stopwords:\n",
    "        flattened_reviews = [word for word in flattened_reviews if \\\n",
    "                             word not in stopwords]\n",
    "\n",
    "\n",
    "    # remove any empty strings that were created by this process\n",
    "    flattened_reviews = [review for review in flattened_reviews if review]\n",
    "\n",
    "    return Counter(flattened_reviews).most_common(n_most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 8125),\n",
       " ('parc', 6350),\n",
       " ('pour', 6085),\n",
       " ('et', 5374),\n",
       " ('les', 3795),\n",
       " ('très', 3631),\n",
       " ('un', 3510),\n",
       " ('beau', 3264),\n",
       " ('la', 3110),\n",
       " ('le', 2947),\n",
       " ('des', 2541),\n",
       " ('en', 2281),\n",
       " ('à', 2241),\n",
       " ('avec', 1934),\n",
       " ('bien', 1706)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMostCommonWords(pos_reviews['french_text'], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 2048),\n",
       " ('parc', 1001),\n",
       " ('les', 930),\n",
       " ('et', 930),\n",
       " ('pour', 920),\n",
       " ('le', 811),\n",
       " ('pas', 736),\n",
       " ('la', 720),\n",
       " ('un', 666),\n",
       " ('des', 499),\n",
       " ('à', 496),\n",
       " ('a', 476),\n",
       " ('mais', 471),\n",
       " ('en', 433),\n",
       " ('il', 426)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMostCommonWords(neg_reviews['french_text'], 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common words involve a lot of common words. Therefore we will look at them without stopwords and determine the most common words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parc', 6350),\n",
       " ('très', 3631),\n",
       " ('beau', 3264),\n",
       " ('bien', 1706),\n",
       " ('endroit', 1702),\n",
       " ('a', 1697),\n",
       " ('enfants', 1500),\n",
       " ('jeux', 1299),\n",
       " ('belle', 1267),\n",
       " ('cest', 1036)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMostCommonWords(pos_reviews['french_text'], 10, stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parc', 1001),\n",
       " ('a', 476),\n",
       " ('très', 387),\n",
       " ('beau', 330),\n",
       " ('cest', 296),\n",
       " ('enfants', 278),\n",
       " ('bien', 269),\n",
       " ('petit', 225),\n",
       " ('beaucoup', 218),\n",
       " ('plus', 206),\n",
       " ('trop', 195),\n",
       " ('jeux', 192),\n",
       " ('endroit', 186),\n",
       " ('terrain', 174),\n",
       " ('peu', 160)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMostCommonWords(neg_reviews['french_text'], 15, stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=15)\n",
    "bow = vectorizer.fit_transform(list(frenchReviewsDf['french_text']))\n",
    "labels = frenchReviewsDf['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17149, 1180)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '12',\n",
       " '15',\n",
       " '18',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '2020',\n",
       " '50',\n",
       " 'abreuvoir',\n",
       " 'abris',\n",
       " 'absolument',\n",
       " 'accessible',\n",
       " 'accessibles',\n",
       " 'accueil',\n",
       " 'accueillant',\n",
       " 'accès',\n",
       " 'achalandé',\n",
       " 'activité',\n",
       " 'activités',\n",
       " 'adapté',\n",
       " 'adaptés',\n",
       " 'admirer',\n",
       " 'adore',\n",
       " 'adorent',\n",
       " 'adoré',\n",
       " 'adultes',\n",
       " 'afin',\n",
       " 'agreable',\n",
       " 'agréable',\n",
       " 'agréables',\n",
       " 'ai',\n",
       " 'ailleurs',\n",
       " 'aime',\n",
       " 'aiment',\n",
       " 'aimer',\n",
       " 'aimé',\n",
       " 'ainsi',\n",
       " 'air',\n",
       " 'aire',\n",
       " 'aires',\n",
       " 'ait',\n",
       " 'alentours',\n",
       " 'aller',\n",
       " 'allez',\n",
       " 'allé',\n",
       " 'allée',\n",
       " 'alors',\n",
       " 'amateurs',\n",
       " 'ambiance',\n",
       " 'amener',\n",
       " 'ami',\n",
       " 'amis',\n",
       " 'amour',\n",
       " 'amoureux',\n",
       " 'amusant',\n",
       " 'amusement',\n",
       " 'amusent',\n",
       " 'amuser',\n",
       " 'amélioration',\n",
       " 'améliorer',\n",
       " 'aménagement',\n",
       " 'aménagements',\n",
       " 'aménager',\n",
       " 'aménagé',\n",
       " 'aménagée',\n",
       " 'aménagés',\n",
       " 'an',\n",
       " 'ancien',\n",
       " 'and',\n",
       " 'angus',\n",
       " 'animation',\n",
       " 'animaux',\n",
       " 'animé',\n",
       " 'anjou',\n",
       " 'année',\n",
       " 'années',\n",
       " 'ans',\n",
       " 'apaisant',\n",
       " 'apporter',\n",
       " 'apprécier',\n",
       " 'apprécié',\n",
       " 'après',\n",
       " 'arbre',\n",
       " 'arbres',\n",
       " 'arc',\n",
       " 'architecture',\n",
       " 'argent',\n",
       " 'arrondissement',\n",
       " 'arrêt',\n",
       " 'arrêter',\n",
       " 'artifice',\n",
       " 'artifices',\n",
       " 'artificiel',\n",
       " 'aréna',\n",
       " 'as',\n",
       " 'asseoir',\n",
       " 'assez',\n",
       " 'assoir',\n",
       " 'athlétisme',\n",
       " 'atmosphère',\n",
       " 'attendre',\n",
       " 'attention',\n",
       " 'au',\n",
       " 'aucun',\n",
       " 'aucune',\n",
       " 'aujourd',\n",
       " 'aurait',\n",
       " 'aussi',\n",
       " 'autant',\n",
       " 'auto',\n",
       " 'autobus',\n",
       " 'automne',\n",
       " 'autoroute',\n",
       " 'autour',\n",
       " 'autre',\n",
       " 'autres',\n",
       " 'aux',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avant',\n",
       " 'avec',\n",
       " 'avez',\n",
       " 'avis',\n",
       " 'avoir',\n",
       " 'avons',\n",
       " 'bac',\n",
       " 'baigner',\n",
       " 'bain',\n",
       " 'balade',\n",
       " 'balader',\n",
       " 'balades',\n",
       " 'balançoire',\n",
       " 'balançoires',\n",
       " 'ball',\n",
       " 'ballade',\n",
       " 'balle',\n",
       " 'ballon',\n",
       " 'banc',\n",
       " 'bancs',\n",
       " 'bar',\n",
       " 'barbecue',\n",
       " 'barboteuse',\n",
       " 'bas',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'basket',\n",
       " 'basketball',\n",
       " 'bassin',\n",
       " 'bateaux',\n",
       " 'bbq',\n",
       " 'bcp',\n",
       " 'beau',\n",
       " 'beaucoup',\n",
       " 'beauté',\n",
       " 'beaux',\n",
       " 'bel',\n",
       " 'belle',\n",
       " 'belles',\n",
       " 'ben',\n",
       " 'besoin',\n",
       " 'bibliothèque',\n",
       " 'bicyclette',\n",
       " 'bien',\n",
       " 'bixi',\n",
       " 'bière',\n",
       " 'boire',\n",
       " 'bois',\n",
       " 'boisé',\n",
       " 'bon',\n",
       " 'bondé',\n",
       " 'bonheur',\n",
       " 'bonne',\n",
       " 'bonnes',\n",
       " 'bons',\n",
       " 'bord',\n",
       " 'bordure',\n",
       " 'bordé',\n",
       " 'botanique',\n",
       " 'bouffe',\n",
       " 'boulevard',\n",
       " 'bout',\n",
       " 'bravo',\n",
       " 'bref',\n",
       " 'bruit',\n",
       " 'bruyant',\n",
       " 'bus',\n",
       " 'but',\n",
       " 'bute',\n",
       " 'butte',\n",
       " 'bémol',\n",
       " 'béton',\n",
       " 'ca',\n",
       " 'cabane',\n",
       " 'cadre',\n",
       " 'café',\n",
       " 'calme',\n",
       " 'campagne',\n",
       " 'canada',\n",
       " 'canal',\n",
       " 'canard',\n",
       " 'canards',\n",
       " 'canicule',\n",
       " 'canin',\n",
       " 'car',\n",
       " 'carré',\n",
       " 'cartier',\n",
       " 'cas',\n",
       " 'cause',\n",
       " 'ce',\n",
       " 'cela',\n",
       " 'celui',\n",
       " 'central',\n",
       " 'centre',\n",
       " 'cependant',\n",
       " 'certain',\n",
       " 'certaines',\n",
       " 'certains',\n",
       " 'ces',\n",
       " 'cest',\n",
       " 'cet',\n",
       " 'cette',\n",
       " 'ceux',\n",
       " 'chaises',\n",
       " 'chalet',\n",
       " 'chaleur',\n",
       " 'chaleureux',\n",
       " 'chance',\n",
       " 'changer',\n",
       " 'chaque',\n",
       " 'charmant',\n",
       " 'charme',\n",
       " 'chaud',\n",
       " 'chaude',\n",
       " 'chaudes',\n",
       " 'chemin',\n",
       " 'chemins',\n",
       " 'cher',\n",
       " 'chevreuils',\n",
       " 'chez',\n",
       " 'chien',\n",
       " 'chiens',\n",
       " 'chiller',\n",
       " 'choix',\n",
       " 'chose',\n",
       " 'choses',\n",
       " 'chouette',\n",
       " 'ci',\n",
       " 'ciel',\n",
       " 'circuit',\n",
       " 'circulation',\n",
       " 'cirque',\n",
       " 'citoyens',\n",
       " 'clôture',\n",
       " 'clôturé',\n",
       " 'coeur',\n",
       " 'coin',\n",
       " 'coins',\n",
       " 'colline',\n",
       " 'comme',\n",
       " 'commun',\n",
       " 'communautaire',\n",
       " 'compagnie',\n",
       " 'complet',\n",
       " 'complètement',\n",
       " 'compte',\n",
       " 'concerts',\n",
       " 'confortable',\n",
       " 'connu',\n",
       " 'conseille',\n",
       " 'construction',\n",
       " 'contre',\n",
       " 'convivial',\n",
       " 'cool',\n",
       " 'correct',\n",
       " 'cote',\n",
       " 'coucher',\n",
       " 'coup',\n",
       " 'couple',\n",
       " 'courir',\n",
       " 'cours',\n",
       " 'course',\n",
       " 'court',\n",
       " 'covid',\n",
       " 'croirait',\n",
       " 'cyclable',\n",
       " 'cyclables',\n",
       " 'cyclistes',\n",
       " 'côte',\n",
       " 'côté',\n",
       " 'cœur',\n",
       " 'dame',\n",
       " 'dangereux',\n",
       " 'dans',\n",
       " 'danse',\n",
       " 'de',\n",
       " 'dehors',\n",
       " 'demande',\n",
       " 'depuis',\n",
       " 'dernier',\n",
       " 'derrière',\n",
       " 'des',\n",
       " 'dessus',\n",
       " 'deux',\n",
       " 'devant',\n",
       " 'devient',\n",
       " 'devrait',\n",
       " 'difficile',\n",
       " 'différentes',\n",
       " 'différents',\n",
       " 'dimanche',\n",
       " 'dire',\n",
       " 'disponible',\n",
       " 'disponibles',\n",
       " 'dispose',\n",
       " 'disposition',\n",
       " 'distanciation',\n",
       " 'dit',\n",
       " 'divers',\n",
       " 'diverses',\n",
       " 'diversité',\n",
       " 'doit',\n",
       " 'dommage',\n",
       " 'donc',\n",
       " 'donne',\n",
       " 'donner',\n",
       " 'dont',\n",
       " 'drapeau',\n",
       " 'drogue',\n",
       " 'du',\n",
       " 'durant',\n",
       " 'début',\n",
       " 'décevant',\n",
       " 'déchets',\n",
       " 'découverte',\n",
       " 'découvrir',\n",
       " 'déjà',\n",
       " 'dépotoir',\n",
       " 'désagréable',\n",
       " 'détendre',\n",
       " 'détente',\n",
       " 'détour',\n",
       " 'déçu',\n",
       " 'dîner',\n",
       " 'dû',\n",
       " 'eau',\n",
       " 'eaux',\n",
       " 'elle',\n",
       " 'elles',\n",
       " 'emmener',\n",
       " 'emplacement',\n",
       " 'en',\n",
       " 'encore',\n",
       " 'endroit',\n",
       " 'endroits',\n",
       " 'enfance',\n",
       " 'enfant',\n",
       " 'enfants',\n",
       " 'enfin',\n",
       " 'ensemble',\n",
       " 'ensoleillé',\n",
       " 'entouré',\n",
       " 'entrainement',\n",
       " 'entraînement',\n",
       " 'entraîner',\n",
       " 'entre',\n",
       " 'entretenu',\n",
       " 'entretenue',\n",
       " 'entretenues',\n",
       " 'entretenus',\n",
       " 'entretien',\n",
       " 'entrée',\n",
       " 'environ',\n",
       " 'environnement',\n",
       " 'environs',\n",
       " 'es',\n",
       " 'espace',\n",
       " 'espaces',\n",
       " 'est',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'ete',\n",
       " 'etre',\n",
       " 'eu',\n",
       " 'eux',\n",
       " 'excellent',\n",
       " 'excellente',\n",
       " 'exceptionnel',\n",
       " 'exemple',\n",
       " 'exercice',\n",
       " 'exercices',\n",
       " 'expérience',\n",
       " 'extraordinaire',\n",
       " 'extérieur',\n",
       " 'extérieure',\n",
       " 'face',\n",
       " 'facile',\n",
       " 'facilement',\n",
       " 'faire',\n",
       " 'fais',\n",
       " 'fait',\n",
       " 'faite',\n",
       " 'faites',\n",
       " 'familial',\n",
       " 'familiale',\n",
       " 'famille',\n",
       " 'familles',\n",
       " 'fantastique',\n",
       " 'faudrait',\n",
       " 'faune',\n",
       " 'faut',\n",
       " 'fer',\n",
       " 'ferme',\n",
       " 'fermer',\n",
       " 'fermé',\n",
       " 'fermée',\n",
       " 'fermés',\n",
       " 'festival',\n",
       " 'festivals',\n",
       " 'feux',\n",
       " 'fi',\n",
       " 'fille',\n",
       " 'fils',\n",
       " 'fin',\n",
       " 'fleurs',\n",
       " 'fleuve',\n",
       " 'flâner',\n",
       " 'fois',\n",
       " 'fond',\n",
       " 'font',\n",
       " 'fontaine',\n",
       " 'fontaines',\n",
       " 'foot',\n",
       " 'football',\n",
       " 'for',\n",
       " 'forme',\n",
       " 'formidable',\n",
       " 'fort',\n",
       " 'fortement',\n",
       " 'forêt',\n",
       " 'frais',\n",
       " 'fraîcheur',\n",
       " 'frisbee',\n",
       " 'fréquenté',\n",
       " 'fumer',\n",
       " 'fun',\n",
       " 'fête',\n",
       " 'gamelin',\n",
       " 'garder',\n",
       " 'gazon',\n",
       " 'gazonné',\n",
       " 'genre',\n",
       " 'gens',\n",
       " 'glace',\n",
       " 'glissade',\n",
       " 'glissades',\n",
       " 'glisse',\n",
       " 'glisser',\n",
       " 'go',\n",
       " 'golf',\n",
       " 'gouin',\n",
       " 'goût',\n",
       " 'goûts',\n",
       " 'grand',\n",
       " 'grande',\n",
       " 'grandes',\n",
       " 'grands',\n",
       " 'gratuit',\n",
       " 'gratuite',\n",
       " 'gratuitement',\n",
       " 'gratuits',\n",
       " 'gros',\n",
       " 'grosse',\n",
       " 'groupe',\n",
       " 'groupes',\n",
       " 'grâce',\n",
       " 'gym',\n",
       " 'génial',\n",
       " 'général',\n",
       " 'généralement',\n",
       " 'habite',\n",
       " 'hamac',\n",
       " 'haut',\n",
       " 'hauteur',\n",
       " 'havre',\n",
       " 'henri',\n",
       " 'herbe',\n",
       " 'heure',\n",
       " 'heures',\n",
       " 'hic',\n",
       " 'histoire',\n",
       " 'historique',\n",
       " 'hiver',\n",
       " 'hivers',\n",
       " 'hochelaga',\n",
       " 'hockey',\n",
       " 'hui',\n",
       " 'hâte',\n",
       " 'ici',\n",
       " 'ideal',\n",
       " 'idéal',\n",
       " 'idée',\n",
       " 'il',\n",
       " 'ile',\n",
       " 'ils',\n",
       " 'ilya',\n",
       " 'immense',\n",
       " 'important',\n",
       " 'importe',\n",
       " 'impossible',\n",
       " 'impression',\n",
       " 'impressionnant',\n",
       " 'in',\n",
       " 'incontournable',\n",
       " 'incroyable',\n",
       " 'infrastructures',\n",
       " 'installation',\n",
       " 'installations',\n",
       " 'installer',\n",
       " 'interdit',\n",
       " 'intéressant',\n",
       " 'intéressante',\n",
       " 'intéressants',\n",
       " 'intérieur',\n",
       " 'intérêt',\n",
       " 'itinérants',\n",
       " 'jacques',\n",
       " 'jadore',\n",
       " 'jai',\n",
       " 'jaime',\n",
       " 'jamais',\n",
       " 'jardin',\n",
       " 'jardins',\n",
       " 'je',\n",
       " 'jean',\n",
       " 'jet',\n",
       " 'jets',\n",
       " 'jeu',\n",
       " 'jeune',\n",
       " 'jeunes',\n",
       " 'jeux',\n",
       " 'job',\n",
       " 'jogging',\n",
       " 'joli',\n",
       " 'jolie',\n",
       " 'joue',\n",
       " 'jouent',\n",
       " 'jouer',\n",
       " 'jour',\n",
       " 'journée',\n",
       " 'journées',\n",
       " 'jours',\n",
       " 'juillet',\n",
       " 'jusqu',\n",
       " 'juste',\n",
       " 'kayak',\n",
       " 'kiosque',\n",
       " 'km',\n",
       " 'la',\n",
       " 'lac',\n",
       " 'lachine',\n",
       " 'lafontaine',\n",
       " 'laisse',\n",
       " 'laissent',\n",
       " 'laisser',\n",
       " 'laurent',\n",
       " 'laurier',\n",
       " 'le',\n",
       " 'lecture',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'leurs',\n",
       " 'libre',\n",
       " 'lieu',\n",
       " 'lieux',\n",
       " 'lire',\n",
       " 'livre',\n",
       " 'location',\n",
       " 'loin',\n",
       " 'loisirs',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'longe',\n",
       " 'longtemps',\n",
       " 'longue',\n",
       " 'longues',\n",
       " 'lors',\n",
       " 'lorsqu',\n",
       " 'lorsque',\n",
       " 'louer',\n",
       " 'louis',\n",
       " 'luge',\n",
       " 'lui',\n",
       " 'lumières',\n",
       " 'lunch',\n",
       " 'là',\n",
       " 'ma',\n",
       " 'magique',\n",
       " 'magnifique',\n",
       " 'magnifiques',\n",
       " 'maintenant',\n",
       " 'mais',\n",
       " 'maison',\n",
       " 'maisonneuve',\n",
       " 'maisons',\n",
       " 'mal',\n",
       " 'malgré',\n",
       " 'malheureusement',\n",
       " 'manger',\n",
       " 'manque',\n",
       " 'marche',\n",
       " 'marcher',\n",
       " 'marches',\n",
       " 'marché',\n",
       " 'match',\n",
       " 'matin',\n",
       " 'matures',\n",
       " 'mauvais',\n",
       " 'mauvaise',\n",
       " 'me',\n",
       " 'meilleur',\n",
       " 'meilleure',\n",
       " 'meilleurs',\n",
       " 'meme',\n",
       " 'merci',\n",
       " 'merveilleux',\n",
       " 'mes',\n",
       " 'metro',\n",
       " 'mettre',\n",
       " 'midi',\n",
       " 'mieux',\n",
       " 'mignon',\n",
       " 'milieu',\n",
       " 'mini',\n",
       " 'minutes',\n",
       " 'mis',\n",
       " 'mise',\n",
       " 'moderne',\n",
       " 'module',\n",
       " 'modules',\n",
       " 'moi',\n",
       " 'moins',\n",
       " 'mois',\n",
       " 'molle',\n",
       " 'moment',\n",
       " 'moments',\n",
       " 'mon',\n",
       " 'monde',\n",
       " 'mont',\n",
       " 'montagne',\n",
       " 'montreal',\n",
       " 'montréal',\n",
       " 'montréalais',\n",
       " 'moustiques',\n",
       " 'moutons',\n",
       " 'moyen',\n",
       " 'mtl',\n",
       " 'multi',\n",
       " 'multiples',\n",
       " 'mur',\n",
       " 'musique',\n",
       " 'mémoire',\n",
       " 'métro',\n",
       " 'même',\n",
       " 'nature',\n",
       " 'naturel',\n",
       " 'ne',\n",
       " 'neige',\n",
       " 'neiges',\n",
       " 'neuf',\n",
       " 'ni',\n",
       " 'nic',\n",
       " 'nice',\n",
       " 'nique',\n",
       " 'niquer',\n",
       " 'niques',\n",
       " 'niveau',\n",
       " 'nom',\n",
       " 'nombre',\n",
       " 'nombreuses',\n",
       " 'nombreux',\n",
       " 'non',\n",
       " 'nord',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nourriture',\n",
       " 'nous',\n",
       " 'nouveau',\n",
       " 'nouveaux',\n",
       " 'nouvelle',\n",
       " 'nouvelles',\n",
       " 'nuit',\n",
       " 'négatif',\n",
       " 'oasis',\n",
       " 'observer',\n",
       " 'occasion',\n",
       " 'occupé',\n",
       " 'odeur',\n",
       " 'of',\n",
       " 'offre',\n",
       " 'oiseaux',\n",
       " 'ok',\n",
       " 'olympique',\n",
       " 'ombragé',\n",
       " 'ombre',\n",
       " 'on',\n",
       " 'ont',\n",
       " 'ordinaire',\n",
       " 'organiser',\n",
       " 'organisé',\n",
       " 'original',\n",
       " 'ou',\n",
       " 'oublier',\n",
       " 'ouest',\n",
       " 'oui',\n",
       " 'outremont',\n",
       " 'ouvert',\n",
       " 'ouverte',\n",
       " 'ouvertes',\n",
       " 'ouverture',\n",
       " 'où',\n",
       " 'paisible',\n",
       " 'paix',\n",
       " 'panier',\n",
       " 'par',\n",
       " 'paradis',\n",
       " 'parc',\n",
       " 'parce',\n",
       " 'parcours',\n",
       " 'parcs',\n",
       " 'parents',\n",
       " 'parfait',\n",
       " 'parfaite',\n",
       " 'parfois',\n",
       " 'park',\n",
       " 'parking',\n",
       " 'part',\n",
       " 'partager',\n",
       " 'particulièrement',\n",
       " 'partie',\n",
       " 'parties',\n",
       " 'partir',\n",
       " 'partout',\n",
       " 'pas',\n",
       " 'passage',\n",
       " 'passe',\n",
       " 'passent',\n",
       " 'passer',\n",
       " 'passerelle',\n",
       " 'passé',\n",
       " 'pataugeoire',\n",
       " 'patin',\n",
       " 'patinage',\n",
       " 'patiner',\n",
       " 'patinoire',\n",
       " 'patinoires',\n",
       " 'patins',\n",
       " 'pause',\n",
       " 'pavillon',\n",
       " 'payant',\n",
       " 'paysage',\n",
       " 'paysages',\n",
       " 'peine',\n",
       " 'pelouse',\n",
       " 'pendant',\n",
       " 'pense',\n",
       " 'penser',\n",
       " 'pente',\n",
       " 'permet',\n",
       " 'personne',\n",
       " 'personnel',\n",
       " 'personnes',\n",
       " 'petit',\n",
       " 'petite',\n",
       " 'petites',\n",
       " 'petits',\n",
       " 'peu',\n",
       " 'peut',\n",
       " 'peuvent',\n",
       " 'peux',\n",
       " 'photo',\n",
       " 'photos',\n",
       " 'physique',\n",
       " 'piano',\n",
       " 'pic',\n",
       " 'picnic',\n",
       " 'picnics',\n",
       " 'picniquer',\n",
       " 'pied',\n",
       " 'pieds',\n",
       " 'pierre',\n",
       " 'ping',\n",
       " 'pique',\n",
       " 'pire',\n",
       " 'pis',\n",
       " 'piscine',\n",
       " 'piste',\n",
       " 'pistes',\n",
       " 'piétons',\n",
       " 'place',\n",
       " 'places',\n",
       " 'plage',\n",
       " 'plaisant',\n",
       " 'plaisir',\n",
       " 'plan',\n",
       " 'plate',\n",
       " 'plateau',\n",
       " 'plein',\n",
       " 'pleine',\n",
       " 'pleins',\n",
       " 'plus',\n",
       " 'plusieurs',\n",
       " 'plutôt',\n",
       " 'point',\n",
       " 'points',\n",
       " 'police',\n",
       " 'pong',\n",
       " 'pont',\n",
       " 'populaire',\n",
       " 'port',\n",
       " 'porte',\n",
       " 'poser',\n",
       " 'possibilité',\n",
       " 'possible',\n",
       " 'potentiel',\n",
       " 'poubelle',\n",
       " 'poubelles',\n",
       " 'pour',\n",
       " 'pourquoi',\n",
       " 'pourrait',\n",
       " 'pourtant',\n",
       " 'pouvez',\n",
       " 'pouvoir',\n",
       " 'prairies',\n",
       " 'pratique',\n",
       " 'pratiquer',\n",
       " 'premier',\n",
       " 'première',\n",
       " 'prend',\n",
       " 'prendre',\n",
       " 'pres',\n",
       " 'presque',\n",
       " 'printemps',\n",
       " 'pris',\n",
       " 'prix',\n",
       " 'probablement',\n",
       " 'problème',\n",
       " 'proche',\n",
       " 'profiter',\n",
       " 'projet',\n",
       " 'promenade',\n",
       " 'promenades',\n",
       " 'promener',\n",
       " 'promène',\n",
       " 'propice',\n",
       " 'propre',\n",
       " 'propres',\n",
       " 'propreté',\n",
       " 'proximité',\n",
       " 'près',\n",
       " 'préféré',\n",
       " 'présence',\n",
       " 'présentement',\n",
       " 'prêt',\n",
       " 'pu',\n",
       " 'public',\n",
       " 'publique',\n",
       " 'publiques',\n",
       " 'puis',\n",
       " 'puisqu',\n",
       " 'pédestre',\n",
       " 'période',\n",
       " 'pétanque',\n",
       " 'pêche',\n",
       " 'pêcher',\n",
       " 'qu',\n",
       " 'quai',\n",
       " 'qualité',\n",
       " 'quand',\n",
       " 'quantité',\n",
       " 'quartier',\n",
       " 'que',\n",
       " 'quel',\n",
       " 'quelle',\n",
       " 'quelque',\n",
       " 'quelques',\n",
       " 'qui',\n",
       " 'quoi',\n",
       " 'québec',\n",
       " 'rafraîchir',\n",
       " 'raison',\n",
       " 'randonnée',\n",
       " 'randonnées',\n",
       " 'rapide',\n",
       " 'rapides',\n",
       " 'raquette',\n",
       " 'raquettes',\n",
       " 'rare',\n",
       " 'rassemblement',\n",
       " 'recommande',\n",
       " 'refaire',\n",
       " 'refait',\n",
       " 'regarder',\n",
       " 'relativement',\n",
       " 'relax',\n",
       " 'relaxant',\n",
       " 'relaxe',\n",
       " 'relaxer',\n",
       " 'rempli',\n",
       " 'rencontre',\n",
       " 'rencontrer',\n",
       " 'rencontres',\n",
       " 'rend',\n",
       " 'rendez',\n",
       " 'rendre',\n",
       " 'repas',\n",
       " 'repos',\n",
       " 'reposant',\n",
       " 'reposer',\n",
       " 'respect',\n",
       " 'ressourcer',\n",
       " 'reste',\n",
       " 'rester',\n",
       " 'resto',\n",
       " 'retourner',\n",
       " 'retrouve',\n",
       " 'retrouver',\n",
       " 'rien',\n",
       " 'rive',\n",
       " 'rivière',\n",
       " 'romantique',\n",
       " 'ronde',\n",
       " 'rosemont',\n",
       " 'route',\n",
       " 'royal',\n",
       " 'rue',\n",
       " 'rues',\n",
       " 'ruisseau',\n",
       " 'réaménagement',\n",
       " 'réaménagé',\n",
       " 'récemment',\n",
       " 'régulièrement',\n",
       " 'rénovation',\n",
       " 'rénovations',\n",
       " 'rénové',\n",
       " 'résidents',\n",
       " 'sa',\n",
       " 'sable',\n",
       " 'saint',\n",
       " 'sais',\n",
       " 'saison',\n",
       " 'saisons',\n",
       " 'sale',\n",
       " 'salle',\n",
       " 'salles',\n",
       " 'samedi',\n",
       " 'sanitaires',\n",
       " 'sans',\n",
       " 'santé',\n",
       " 'sauf',\n",
       " 'savoir',\n",
       " 'sculptures',\n",
       " 'scène',\n",
       " 'se',\n",
       " 'secteur',\n",
       " 'section',\n",
       " 'sections',\n",
       " 'selon',\n",
       " 'semaine',\n",
       " 'semble',\n",
       " 'sens',\n",
       " 'sent',\n",
       " 'sentier',\n",
       " 'sentiers',\n",
       " 'sera',\n",
       " 'serait',\n",
       " 'service',\n",
       " 'services',\n",
       " 'ses',\n",
       " 'seul',\n",
       " 'seule',\n",
       " 'seulement',\n",
       " 'si',\n",
       " 'simple',\n",
       " 'simplement',\n",
       " 'sinon',\n",
       " 'site',\n",
       " 'situé',\n",
       " 'skate',\n",
       " 'skatepark',\n",
       " 'ski',\n",
       " 'soccer',\n",
       " 'soient',\n",
       " 'soir',\n",
       " 'soirs',\n",
       " 'soirée',\n",
       " 'soit',\n",
       " 'sol',\n",
       " 'soleil',\n",
       " 'sommes',\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1180"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfDict = dict(zip(vectorizer.get_feature_names(), bow.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfDict['améliorer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDf = pd.DataFrame.from_dict(tfidfDict, \n",
    "                                   orient='index', columns=['tfidf'])\n",
    "featureDf.reset_index(inplace=True)\n",
    "featureDf = featureDf.rename(columns = {'index':'feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>pour</td>\n",
       "      <td>0.162610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>et</td>\n",
       "      <td>0.173683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>beau</td>\n",
       "      <td>0.197146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>un</td>\n",
       "      <td>0.200496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>faire</td>\n",
       "      <td>0.308118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>de</td>\n",
       "      <td>0.313332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>tres</td>\n",
       "      <td>0.359868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>nature</td>\n",
       "      <td>0.371318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>la</td>\n",
       "      <td>0.425115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>profiter</td>\n",
       "      <td>0.447984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature     tfidf\n",
       "827       pour  0.162610\n",
       "379         et  0.173683\n",
       "152       beau  0.197146\n",
       "1089        un  0.200496\n",
       "398      faire  0.308118\n",
       "295         de  0.313332\n",
       "1079      tres  0.359868\n",
       "664     nature  0.371318\n",
       "558         la  0.425115\n",
       "848   profiter  0.447984"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureDf.sort_values('tfidf')[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the words that have the highest tfidf score in the positive and negative sentiment datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_pos = TfidfVectorizer(min_df=15)\n",
    "bow_pos = vectorizer_pos.fit_transform(list(pos_reviews['french_text']))\n",
    "labels_pos = pos_reviews['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_neg = TfidfVectorizer(min_df=15)\n",
    "bow_neg = vectorizer_neg.fit_transform(list(neg_reviews['french_text']))\n",
    "labels_neg = neg_reviews['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfDictPos = dict(zip(vectorizer_pos.get_feature_names(), bow_pos.toarray()[0]))\n",
    "tfidfDictNeg = dict(zip(vectorizer_neg.get_feature_names(), bow_neg.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "posFeatureDf = pd.DataFrame.from_dict(tfidfDictPos, \n",
    "                                   orient='index', columns=['tfidf'])\n",
    "posFeatureDf.reset_index(inplace=True)\n",
    "posFeatureDf = posFeatureDf.rename(columns = {'index':'feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "negFeatureDf = pd.DataFrame.from_dict(tfidfDictNeg, \n",
    "                                   orient='index', columns=['tfidf'])\n",
    "negFeatureDf.reset_index(inplace=True)\n",
    "negFeatureDf = negFeatureDf.rename(columns = {'index':'feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>faites</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>familial</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>eu</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>ete</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>parc</td>\n",
       "      <td>0.149239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>pour</td>\n",
       "      <td>0.161498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>et</td>\n",
       "      <td>0.173906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>beau</td>\n",
       "      <td>0.193378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>un</td>\n",
       "      <td>0.202580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>faire</td>\n",
       "      <td>0.309099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>de</td>\n",
       "      <td>0.323187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>tres</td>\n",
       "      <td>0.356667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>nature</td>\n",
       "      <td>0.365215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>la</td>\n",
       "      <td>0.432156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>profiter</td>\n",
       "      <td>0.442295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature     tfidf\n",
       "339    faites  0.000000\n",
       "340  familial  0.000000\n",
       "323        eu  0.000000\n",
       "322       ete  0.000000\n",
       "625      parc  0.149239\n",
       "712      pour  0.161498\n",
       "320        et  0.173906\n",
       "129      beau  0.193378\n",
       "930        un  0.202580\n",
       "336     faire  0.309099\n",
       "254        de  0.323187\n",
       "922      tres  0.356667\n",
       "571    nature  0.365215\n",
       "472        la  0.432156\n",
       "729  profiter  0.442295"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posFeatureDf.sort_values('tfidf')[-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>gens</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>gazon</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>fait</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>être</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>fontaine</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>font</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>fois</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>fleuve</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>fin</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>fermé</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>faut</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>football</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>de</td>\n",
       "      <td>0.254753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>trop</td>\n",
       "      <td>0.515403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>sentier</td>\n",
       "      <td>0.818206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature     tfidf\n",
       "108      gens  0.000000\n",
       "107     gazon  0.000000\n",
       "97       fait  0.000000\n",
       "313      être  0.000000\n",
       "105  fontaine  0.000000\n",
       "104      font  0.000000\n",
       "103      fois  0.000000\n",
       "102    fleuve  0.000000\n",
       "101       fin  0.000000\n",
       "100     fermé  0.000000\n",
       "99       faut  0.000000\n",
       "106  football  0.000000\n",
       "68         de  0.254753\n",
       "288      trop  0.515403\n",
       "241   sentier  0.818206"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negFeatureDf.sort_values('tfidf')[-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 200 best features \n",
    "selected_features = SelectKBest(chi2, k=200).fit(bow, labels).get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<41822x200 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use selected features for vectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=15, vocabulary=selected_features)\n",
    "\n",
    "bow2 = vectorizer.fit_transform(list(parkReviews['review_text']))\n",
    "bow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20,\n",
       " 37,\n",
       " 76,\n",
       " 85,\n",
       " 93,\n",
       " 109,\n",
       " 124,\n",
       " 130,\n",
       " 140,\n",
       " 141,\n",
       " 143,\n",
       " 149,\n",
       " 150,\n",
       " 164,\n",
       " 173,\n",
       " 177,\n",
       " 182,\n",
       " 185,\n",
       " 201,\n",
       " 202,\n",
       " 215,\n",
       " 220,\n",
       " 238,\n",
       " 243,\n",
       " 262,\n",
       " 275,\n",
       " 295,\n",
       " 297,\n",
       " 299,\n",
       " 301,\n",
       " 343,\n",
       " 353,\n",
       " 355,\n",
       " 363,\n",
       " 383,\n",
       " 392,\n",
       " 396,\n",
       " 397,\n",
       " 407,\n",
       " 411,\n",
       " 436,\n",
       " 443,\n",
       " 450,\n",
       " 454,\n",
       " 464,\n",
       " 478,\n",
       " 486,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 500,\n",
       " 511,\n",
       " 512,\n",
       " 515,\n",
       " 518,\n",
       " 533,\n",
       " 534,\n",
       " 540,\n",
       " 541,\n",
       " 564,\n",
       " 574,\n",
       " 606,\n",
       " 608,\n",
       " 609,\n",
       " 618,\n",
       " 639,\n",
       " 684,\n",
       " 701,\n",
       " 721,\n",
       " 728,\n",
       " 729,\n",
       " 736,\n",
       " 748,\n",
       " 761,\n",
       " 767,\n",
       " 779,\n",
       " 788,\n",
       " 816,\n",
       " 821,\n",
       " 843,\n",
       " 861,\n",
       " 864,\n",
       " 873,\n",
       " 892,\n",
       " 913,\n",
       " 919,\n",
       " 922,\n",
       " 924,\n",
       " 929,\n",
       " 962,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 1004,\n",
       " 1005,\n",
       " 1007,\n",
       " 1011,\n",
       " 1062,\n",
       " 1063,\n",
       " 1064,\n",
       " 1077,\n",
       " 1082,\n",
       " 1092,\n",
       " 1101,\n",
       " 1148,\n",
       " 1152,\n",
       " 1158,\n",
       " 1174,\n",
       " 1192,\n",
       " 1198,\n",
       " 1200,\n",
       " 1206,\n",
       " 1211,\n",
       " 1219,\n",
       " 1220,\n",
       " 1221,\n",
       " 1222,\n",
       " 1226,\n",
       " 1228,\n",
       " 1246,\n",
       " 1248,\n",
       " 1249,\n",
       " 1258,\n",
       " 1271,\n",
       " 1292,\n",
       " 1301,\n",
       " 1309,\n",
       " 1331,\n",
       " 1336,\n",
       " 1338,\n",
       " 1355,\n",
       " 1374,\n",
       " 1399,\n",
       " 1405,\n",
       " 1406,\n",
       " 1407,\n",
       " 1418,\n",
       " 1434,\n",
       " 1441,\n",
       " 1453,\n",
       " 1473,\n",
       " 1498,\n",
       " 1501,\n",
       " 1504,\n",
       " 1509,\n",
       " 1567,\n",
       " 1640,\n",
       " 1670,\n",
       " 1686,\n",
       " 1688,\n",
       " 1689,\n",
       " 1690,\n",
       " 1692,\n",
       " 1725,\n",
       " 1759,\n",
       " 1769,\n",
       " 1814,\n",
       " 1815,\n",
       " 1816,\n",
       " 1858,\n",
       " 1861,\n",
       " 1862,\n",
       " 1863,\n",
       " 1867,\n",
       " 1871,\n",
       " 1899,\n",
       " 1900,\n",
       " 1901,\n",
       " 1903,\n",
       " 1918,\n",
       " 1929,\n",
       " 1953,\n",
       " 1955,\n",
       " 1958,\n",
       " 1962,\n",
       " 1970,\n",
       " 1971,\n",
       " 1985,\n",
       " 1991,\n",
       " 2006,\n",
       " 2020,\n",
       " 2024,\n",
       " 2025,\n",
       " 2038,\n",
       " 2043,\n",
       " 2049,\n",
       " 2054,\n",
       " 2067,\n",
       " 2069,\n",
       " 2083,\n",
       " 2085,\n",
       " 2088,\n",
       " 2096,\n",
       " 2102,\n",
       " 2103,\n",
       " 2106,\n",
       " 2107,\n",
       " 2108,\n",
       " 2109]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([20, 37, 76, 85, 93, 109, 124, 130, 140, 141, 143, 149, 150, 164, 173, 177, 182, 185, 201, 202, 215, 220, 238, 243, 262, 275, 295, 297, 299, 301, 343, 353, 355, 363, 383, 392, 396, 397, 407, 411, 436, 443, 450, 454, 464, 478, 486, 493, 494, 495, 496, 500, 511, 512, 515, 518, 533, 534, 540, 541, 564, 574, 606, 608, 609, 618, 639, 684, 701, 721, 728, 729, 736, 748, 761, 767, 779, 788, 816, 821, 843, 861, 864, 873, 892, 913, 919, 922, 924, 929, 962, 967, 968, 969, 1004, 1005, 1007, 1011, 1062, 1063, 1064, 1077, 1082, 1092, 1101, 1148, 1152, 1158, 1174, 1192, 1198, 1200, 1206, 1211, 1219, 1220, 1221, 1222, 1226, 1228, 1246, 1248, 1249, 1258, 1271, 1292, 1301, 1309, 1331, 1336, 1338, 1355, 1374, 1399, 1405, 1406, 1407, 1418, 1434, 1441, 1453, 1473, 1498, 1501, 1504, 1509, 1567, 1640, 1670, 1686, 1688, 1689, 1690, 1692, 1725, 1759, 1769, 1814, 1815, 1816, 1858, 1861, 1862, 1863, 1867, 1871, 1899, 1900, 1901, 1903, 1918, 1929, 1953, 1955, 1958, 1962, 1970, 1971, 1985, 1991, 2006, 2020, 2024, 2025, 2038, 2043, 2049, 2054, 2067, 2069, 2083, 2085, 2088, 2096, 2102, 2103, 2106, 2107, 2108, 2109])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow, labels, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11489, 1180)\n",
      "(5660, 1180)\n",
      "(11489,)\n",
      "(5660,)\n"
     ]
    }
   ],
   "source": [
    "# check out the dataset \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Forest classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8740282685512367"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = rfc()\n",
    "classifier.fit(X_train,y_train)\n",
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"classifier = rfc()\\n\\nhyperparameters = {\\n    'n_estimators':stats.randint(10,300),\\n    'criterion':['gini','entropy'],\\n    'min_samples_split':stats.randint(2,9),\\n    'bootstrap':[True,False]\\n}\\n\\nrandom_search = RandomizedSearchCV(classifier, hyperparameters, n_iter=65, n_jobs=4)\\n\\nrandom_search.fit(bow, labels)\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = rfc()\n",
    "\n",
    "hyperparameters = {\n",
    "    'n_estimators':stats.randint(10,300),\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split':stats.randint(2,9),\n",
    "    'bootstrap':[True,False]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(classifier, hyperparameters, n_iter=65, n_jobs=4)\n",
    "\n",
    "random_search.fit(bow, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_classifier = random_search.best_estimator_\n",
    "optimized_classifier.fit(X_train,y_train)\n",
    "\n",
    "optimized_classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train shape: \" + str(X_train.shape))\n",
    "print(\"score on test: \" + str(optimized_classifier.score(X_test, y_test)))\n",
    "print(\"score on train: \"+ str(optimized_classifier.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking a look at classification errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"optimized_classifier.fit(X_train,y_train)\\n\\ncorrectly_classified = {}\\nincorrectly_classified = {}\\n\\nfor index, row in enumerate(X_test):\\n    probability = optimized_classifier.predict_proba(row)\\n\\n    # get the location of the review in the dataframe\\n    review_loc = y_test.index[index]\\n\\n    if optimized_classifier.predict(row) == y_test.iloc[index]:\\n        correctly_classified[parkReviews['review_text'].loc[review_loc]] = probability\\n    else:\\n        incorrectly_classified[parkReviews['review_text'].iloc[review_loc]] = probability\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_classifier.fit(X_train,y_train)\n",
    "\n",
    "correctly_classified = {}\n",
    "incorrectly_classified = {}\n",
    "\n",
    "for index, row in enumerate(X_test):\n",
    "    probability = optimized_classifier.predict_proba(row)\n",
    "\n",
    "    # get the location of the review in the dataframe\n",
    "    review_loc = y_test.index[index]\n",
    "\n",
    "    if optimized_classifier.predict(row) == y_test.iloc[index]:\n",
    "        correctly_classified[parkReviews['review_text'].loc[review_loc]] = probability\n",
    "    else:\n",
    "        incorrectly_classified[parkReviews['review_text'].iloc[review_loc]] = probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review, score in incorrectly_classified.items():\n",
    "    print('{}: {}'.format(review, score[0]))\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review, score in correctly_classified.items():\n",
    "    print('{}: {}'.format(review, score[0]))\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " incorrectly_classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (11489, 1180)\n",
      "score on test: 0.8643109540636043\n",
      "score on train: 0.8885890852119419\n",
      "CPU times: user 200 ms, sys: 13.1 ms, total: 213 ms\n",
      "Wall time: 228 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clfdt = DecisionTreeClassifier(min_samples_split=30,max_depth=10)\n",
    "clfdt.fit(X_train, y_train)\n",
    "\n",
    "print(\"train shape: \" + str(X_train.shape))\n",
    "print(\"score on test: \"  + str(clfdt.score(X_test, y_test)))\n",
    "print(\"score on train: \" + str(clfdt.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (11489, 1180)\n",
      "score on test: 0.8591872791519435\n",
      "score on train: 0.8603881974062146\n",
      "CPU times: user 463 ms, sys: 14.3 ms, total: 477 ms\n",
      "Wall time: 507 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bg=BaggingClassifier(DecisionTreeClassifier(min_samples_split=10,max_depth=3),max_samples=0.5,max_features=1.0,n_estimators=10)\n",
    "bg.fit(X_train, y_train)\n",
    "\n",
    "print(\"train shape: \" + str(X_train.shape))\n",
    "print(\"score on test: \" + str(bg.score(X_test, y_test)))\n",
    "print(\"score on train: \"+ str(bg.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (11489, 1180)\n",
      "score on test: 0.8743816254416961\n",
      "score on train: 0.8938114718426321\n"
     ]
    }
   ],
   "source": [
    "# boosting decision tree\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# setting \n",
    "# min_samples_split=10\n",
    "# max_depth=4\n",
    "\n",
    "adb = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),n_estimators=100,learning_rate=0.5)\n",
    "adb.fit(X_train, y_train)\n",
    "\n",
    "print(\"train shape: \" + str(X_train.shape))\n",
    "print(\"score on test: \" + str(adb.score(X_test, y_test)))\n",
    "print(\"score on train: \"+ str(adb.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1180 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Sklearn Documentation:\n",
    "\n",
    "- Naive Bayes: https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "- MultinomialNB: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.85 ms, sys: 11.9 ms, total: 18.7 ms\n",
      "Wall time: 36.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mnb = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (11489, 1180)\n",
      "score on test: 0.873321554770318\n",
      "score on train: 0.8766646357385325\n"
     ]
    }
   ],
   "source": [
    "print(\"train shape: \" + str(X_train.shape))\n",
    "print(\"score on test: \" + str(mnb.score(X_test, y_test)))\n",
    "print(\"score on train: \"+ str(mnb.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "\n",
    "Sklearn Documentation:\n",
    "\n",
    "- LogisticRegression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- SGD Classifier: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 513 ms, sys: 69.2 ms, total: 582 ms\n",
      "Wall time: 757 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=5000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr=LogisticRegression(max_iter=5000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (11489, 1180)\n",
      "score on test: 0.8763250883392226\n",
      "score on train: 0.881016624597441\n"
     ]
    }
   ],
   "source": [
    "print(\"train shape: \" + str(X_train.shape))\n",
    "print(\"score on test: \" + str(lr.score(X_test, y_test)))\n",
    "print(\"score on train: \"+ str(lr.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 ms, sys: 7.11 ms, total: 32.9 ms\n",
      "Wall time: 43.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#logistic regression with stochastic gradient decent\n",
    "sgd=SGDClassifier()\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (11489, 1180)\n",
      "score on test: 0.876678445229682\n",
      "score on train: 0.8839759770214988\n"
     ]
    }
   ],
   "source": [
    "print(\"train shape: \" + str(X_train.shape))\n",
    "print(\"score on test: \" + str(sgd.score(X_test, y_test)))\n",
    "print(\"score on train: \"+ str(sgd.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (11489, 1180)\n",
      "score on test: 0.8454063604240283\n",
      "score on train: 0.876403516406998\n",
      "CPU times: user 21.9 s, sys: 12.5 s, total: 34.3 s\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#knn = KNeighborsClassifier(n_neighbors=5,algorithm = 'ball_tree')\n",
    "knn = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"train shape: \" + str(X_train.shape))\n",
    "print(\"score on test: \" + str(knn.score(X_test, y_test)))\n",
    "print(\"score on train: \"+ str(knn.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network pre-programmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split an additional validation dataset\n",
    "X_validation=X_train[:100]\n",
    "X_partial_train=X_train[100:]\n",
    "y_validation=y_train[:100]\n",
    "y_partial_train=y_train[100:]\n",
    "model=models.Sequential()\n",
    "model.add(layers.Dense(16,activation='relu',input_shape=(30,)))\n",
    "model.add(layers.Dense(16,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(X_partial_train,y_partial_train,epochs=4,batch_size=512,validation_data=(X_validation,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('')\n",
    "print(\"train shape: \" + str(x_train.shape))\n",
    "print(\"score on test: \" + str(model.evaluate(x_test,y_test)[1]))\n",
    "print(\"score on train: \"+ str(model.evaluate(x_train,y_train)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#allReviewEn[['']]\n",
    "\n",
    "train, test = train_test_split(allReviewEn, test_size = 0.3, random_state=42)\n",
    "\n",
    "# clean the indexing\n",
    "train.reset_index(drop=True),test.reset_index(drop=True)\n",
    "\n",
    "# save train and test in csv files \n",
    "train[['review_text', 'label']].to_csv('all_en_train.csv', index=False)\n",
    "test[['review_text', 'label']].to_csv('all_en_test.csv', index=False)\n",
    "\n",
    "### Using Torchtest to processs text data\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torchtext\n",
    "\n",
    "from torchtext.legacy.data import Field, BucketIterator, TabularDataset, LabelField\n",
    "\n",
    "import nltk \n",
    "nltk.download('punkt') # for punkt tokenizer\n",
    "\n",
    "from nltk import word_tokenize \n",
    "\n",
    "# torchtext field parameter specifies how data should be processed, here tokenized\n",
    "TEXT = Field(tokenize = word_tokenize)\n",
    "\n",
    "LABEL = LabelField(dtype = torch.float) # convert \n",
    "\n",
    "datafields = [ ('review_text', TEXT), ('label', LABEL)] \n",
    "\n",
    "# specify what data that will work with, split to train and text, map to field \n",
    "trn, tst = TabularDataset.splits(path = '/Users/andreamock/Documents/review_datasets',\n",
    "                               train = 'all_en_train.csv', test = 'all_en_test.csv', format = 'csv',\n",
    "                               skip_header = True, fields = datafields)\n",
    "\n",
    "\n",
    "# training examples \n",
    "trn[:5]\n",
    "\n",
    "print(f'Number of training examples: {len(trn)}')\n",
    "print(f'Number of testing examples: {len(tst)}')\n",
    "\n",
    "# each example has label and text\n",
    "trn[5].__dict__.keys()\n",
    "\n",
    "trn[1].review_text # text has been tokenized in individual words\n",
    "\n",
    "trn[1].label\n",
    "\n",
    "# limit size of feature vectors to 15000, use one-encoding to get the top 15000 words in vocab\n",
    "TEXT.build_vocab(trn, max_size = 15000)\n",
    "\n",
    "LABEL.build_vocab(trn)\n",
    "\n",
    "print(f'Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}')\n",
    "print(f'Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}')\n",
    "# two additional tokens were added to vocab, one for unknown words and another for padding to make sentences equal lengths\n",
    "\n",
    "print(TEXT.vocab.freqs.most_common(50)) \n",
    "\n",
    "print(TEXT.vocab.itos[:10]) # integer to string mapping 0 and 1 to unknown and padding\n",
    "\n",
    "batch_size = 64 \n",
    "\n",
    "# returns a batch of examples where each example is of similar length (thus minimizing padding for each example)\n",
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (trn,tst), batch_size = batch_size, sort_key = lambda x: len(x.review_text), sort_within_batch = False\n",
    ")\n",
    "\n",
    "## Designing an RNN for binary text classification \n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        # input_dim = input dimensions of words \n",
    "        # embedding_dim = dimension of word embeddings, dense word representation for training RNN\n",
    "        # hidden_dim = dimension of hidden state of RNN\n",
    "        # output_dim = output dimensions of RNN output\n",
    "        \n",
    "        super().__init__()\n",
    "        #  convert one-hot encoded sentences to dense format using embeddings to represent each word\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        # input to rnn is current word's embedding and previous hidden state, one word per time instance (memory cell)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # fully connected layer to classify as positive or negative \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # input sentence (list of indexes of one hot encoded words) is represented using its embedding\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        embedded_dropout = self.dropout(embedded)\n",
    "        \n",
    "        # output = concatentation of hidden state for every time step (ie word) [sentence length, batch size, hiddendim]\n",
    "        # hidden = final hidden state fed into linear layer\n",
    "        output, (hidden, _) = self.rnn(embedded_dropout)\n",
    "        \n",
    "        hidden_1D = hidden.squeeze(0) # get rid of unnecessary dimension \n",
    "        \n",
    "        assert torch.equal(output[-1, :, :], hidden_1D) # confirm that it is indeed last hidden state \n",
    "        \n",
    "        return self.fc(hidden_1D) # last hidden state fed into fully connected layer\n",
    "\n",
    "# setting dimensions \n",
    "input_dim = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim=1\n",
    "\n",
    "model = RNN(input_dim, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "model # see what our model looks like\n",
    "\n",
    "# train with optimizer\n",
    "import torch.optim as optim \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "# binary cross entropy with logits (cross-entropy for binary classification, \n",
    "# w/ sigmoid activation func to predict in range of 0 and 1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train(model, iterator, optimizer, criterion): # helper function for training process\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:  # iterator over all batches of training data\n",
    "        \n",
    "        optimizer.zero_grad() # zero out gradients of optimizer\n",
    "                \n",
    "        predictions = model(batch.review_text).squeeze(1) # make predictions, squeeze to be 1d instead of [, ]\n",
    "        \n",
    "        loss = criterion(predictions, batch.label) # calculate loss\n",
    "        \n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (rounded_preds == batch.label).float() # how many were correct\n",
    "        \n",
    "        acc = correct.sum() / len(correct)\n",
    "        \n",
    "        loss.backward() # backward pass on rnn\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() # keep track of epoch loss and accuracy\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    print(f' Epoch: {epoch+1}, Train loss: {train_loss}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "Now we can test the accuracy on our test data.\n",
    "\n",
    "# don't want to update the parameters when evaluating the accuracy\n",
    "epoch_loss = 0\n",
    "epoch_acc = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_iterator:\n",
    "\n",
    "        predictions = model(batch.review_text).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, batch.label)\n",
    "\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "        \n",
    "        correct = (rounded_preds == batch.label).float() \n",
    "        acc = correct.sum() / len(correct)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "test_loss = epoch_loss / len(test_iterator)\n",
    "test_acc  = epoch_acc / len(test_iterator)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
